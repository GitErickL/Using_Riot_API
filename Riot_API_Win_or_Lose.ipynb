{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import riotwatcher\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving User Data from Riot API Using Riotwatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the Python package riotwatcher (https://github.com/pseudonym117/Riot-Watcher) to utilize Riot's API (https://developer.riotgames.com/)and get relevant data of a player and their matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance of LolWatcher, which is an interface between Python and Riot's API, where we can call various methods to look at data from Riot's API. To do this we need an api_key from Riot games. You can do this by going on the Riot API website provided in the cell above. The API key is what let's riot know who is accessing their data. API keys should not be shared with anyone. If you are going to use riotwatcher code (or some version of this code) for anything, make sure you edit out your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "watcher = riotwatcher.LolWatcher(api_key = 'RGAPI-48517173-a31d-40b7-9425-eec44901d018')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use riotwatcher to get identifying information on Doublelift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weird Caveats to summoners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried grabbing data for Doublelift and found that the data i grabbed did not match up with that of OP.gg or U.gg.\n",
    "\n",
    "The last match that they have is from 3 days ago. I believe that they are not able to grab data as frequently as I expected, at least for today for whatever reason. I have no idea why this is the case.\n",
    "\n",
    "Knowing this, we will only compare our data acquired to sites like OP.gg and U.gg. This will be done outside this notebook and I encourage you to cross-validate your data from these sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1-lKe0sKHoomm1cGKTMGm3usHkmg0g-WD420VYOlLW_MS1I',\n",
       " 'accountId': 'TLGnXMAPXufbdp4-QHxRNZH4YxgyOh3bvVyr0KEs8ao2-Ac',\n",
       " 'puuid': 'IhFJhHpKEN_vNez9P-N98_45C83HsUxDdyTAWrJYaoDdCt4SN0TPxeQ96rar0hT9njemIRezJ2gQrQ',\n",
       " 'name': 'puddingpiler',\n",
       " 'profileIconId': 4052,\n",
       " 'revisionDate': 1647387185000,\n",
       " 'summonerLevel': 413}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We will grab username of the player of interest\n",
    "summoner_name = 'puddingpiler'\n",
    "\n",
    "#We can use the LolWatcher instance to get some data on the user's identifying information\n",
    "watcher.summoner.by_name(region = 'na1', summoner_name = summoner_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use riotwatcher to retrieve data from Riot Games. The data we will be retrieving here is data from the past 100 matches of the player Doublelift. Riotwatcher can only download 100 games per one call of watcher.match.matchlist_by_puuid, so that is the amount of games that we will analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the puuid, a string identifying a player\n",
    "my_puuid = watcher.summoner.by_name('na1', summoner_name)['puuid']\n",
    "\n",
    "#How many matches do you want the data from (max is 100 for LolWatcher)\n",
    "count = 10\n",
    "\n",
    "\n",
    "#Grab list of {count} last match id's\n",
    "match_list = watcher.match.matchlist_by_puuid(region = 'americas', puuid = my_puuid, count = count)\n",
    "\n",
    "\n",
    "#We will append various data from the matches onto these empty lists\n",
    "match_id = []\n",
    "kills = []\n",
    "deaths = []\n",
    "assists = []\n",
    "gold_total = []\n",
    "queue_id = []\n",
    "win = []\n",
    "\n",
    "\n",
    "#Loop over all these matches \n",
    "for match in match_list:\n",
    "    \n",
    "    #We want to append the match identifier string\n",
    "    match_id.append(match)\n",
    "    \n",
    "        \n",
    "    #Grabs dictionary of match info from Riot API\n",
    "    match_json = watcher.match.by_id('americas', match)\n",
    "    \n",
    "    \n",
    "    #Grabs the part of the dictionary that contains non-metadata\n",
    "    match_details = watcher.match.by_id('americas', match)['info']\n",
    "    \n",
    "    \n",
    "    #Saving the match details to a json file for future use\n",
    "    with open(\"Riot_API_Folder/{}_match_details_{}.json\".format(summoner_name, match), \"w\") as outfile:\n",
    "        json.dump(match_json, outfile)\n",
    "\n",
    "    \n",
    "    #Identify the player of interest in each match\n",
    "    #Since there are 10 players we need a way to identify the data of the player of interest\n",
    "    #I do this by looping through the players' puuid and picking the one that matches the\n",
    "    #puuid of the player of interest\n",
    "    for player in match_details['participants']:\n",
    "\n",
    "        if player['puuid'] == my_puuid:\n",
    "            \n",
    "            #These are the metrics that we are interested in\n",
    "            #There are many metrics provided by Riot's API, and in future projects\n",
    "            #I will be more comprehensive with the features I choose\n",
    "            kills.append(player['kills'])\n",
    "            deaths.append(player['deaths'])\n",
    "            assists.append(player['assists'])\n",
    "            gold_total.append(player['goldEarned'])\n",
    "            queue_id.append(match_details['queueId'])\n",
    "            win.append(player['win'])\n",
    "\n",
    "        else: pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data From JSON Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only run next cell of code if you have downloaded relevant JSON files from cell above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have downloaded the json files from Riot's API from the relevant code above so we can use it later. This cell above takes a little while to run and saving data in its completeness ensures I can go back and do any kind of analysis swiftly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell above we created lists of relevant player data. However, we may want to do some extra analysis on this data and may not want to rerun this cell above. This code cell below walks us through how to create the same lists as the code cell above. Skip this cell if you are creating the lists from the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#Getting the filenames in the folder containing match json files\n",
    "#NOTE: if there are any other files in here OR there are duplicate files, you need to do some cleaning \n",
    "#in that directory\n",
    "filenames = [name for name in os.listdir('Riot_API_Folder/')]\n",
    "\n",
    "\n",
    "#Getting the number of files in the folder containing match json files. \n",
    "file_counts = len(filenames)\n",
    "\n",
    "\n",
    "\n",
    "match_id = []\n",
    "kills = []\n",
    "deaths = []\n",
    "assists = []\n",
    "gold_total = []\n",
    "queue_id = []\n",
    "win = []\n",
    "\n",
    "\n",
    "for i in range(file_counts):\n",
    "    \n",
    "    #with open('Riot_API_Folder/{}'.format(filenames[i])) as match_json:\n",
    "    #    match_json = json.load(match_json)\n",
    "        \n",
    "    match_id.append(filenames[i])\n",
    "    \n",
    "    for player in match_json['info']['participants']:\n",
    "        \n",
    "        if player['puuid'] == my_puuid:\n",
    "            \n",
    "            kills.append(player['kills'])\n",
    "            deaths.append(player['deaths'])\n",
    "            assists.append(player['assists'])\n",
    "            gold_total.append(player['goldEarned'])\n",
    "            queue_id.append(match_details['queueId'])\n",
    "            win.append(player['win'])\n",
    "            \n",
    "    \n",
    "            \n",
    "            #Used for testing if you get the same data from other sources (Like OP.gg or U.gg)\n",
    "            '''print(match_json['info']['participants'][0]['kills'],\n",
    "                  match_json['info']['participants'][0]['deaths'],\n",
    "                  match_json['info']['participants'][0]['assists'])'''\n",
    "            \n",
    "            \n",
    "        else: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Riot's API identifies a game type such as Ranked Solo/Duo, ARAM, Draft through identifying numbers called queue_id.The relevant ones for the data I'll be using are:\n",
    "- 400 -> Draft Pick\n",
    "- 420 -> Ranked Solo/Duo\n",
    "- 450 -> ARAM\n",
    "- 900 -> ARURF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, Riot has an exhaustive list of all available queue ID's found here (https://static.developer.riotgames.com/docs/lol/queues.json). You can write a few lines of code that gets the queue type from the queue id that utilizes this json file, if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will create a new list that replaces these nubmers with string of match type\n",
    "match_type = []\n",
    "\n",
    "for queue in queue_id:\n",
    "    \n",
    "    if queue == 420:\n",
    "        match_type.append('Ranked')\n",
    "    \n",
    "    elif queue == 450:\n",
    "        match_type.append('ARAM')\n",
    "        \n",
    "    elif queue == 400:\n",
    "        match_type.append('Draft')\n",
    "        \n",
    "    elif queue == 900:\n",
    "        match_type.append('ARURF')\n",
    "        \n",
    "    else:\n",
    "        match_type.append(queue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining together all data acquired through LolWatcher\n",
    "df = pd.DataFrame({'match_id': match_id, \n",
    "                   'kills': kills, \n",
    "                   'deaths': deaths, \n",
    "                   'assists': assists, \n",
    "                   'gold_earned': gold_total,\n",
    "                   'match_type': match_type,\n",
    "                   'win': win\n",
    "                  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_id</th>\n",
       "      <th>kills</th>\n",
       "      <th>deaths</th>\n",
       "      <th>assists</th>\n",
       "      <th>gold_earned</th>\n",
       "      <th>match_type</th>\n",
       "      <th>win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.ipynb_checkpoints</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>13868</td>\n",
       "      <td>Draft</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>puddingpiler_match_details_NA1_4245705629.json</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>13868</td>\n",
       "      <td>Draft</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>puddingpiler_match_details_NA1_4245731071.json</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>13868</td>\n",
       "      <td>Draft</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>puddingpiler_match_details_NA1_4246871711.json</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>13868</td>\n",
       "      <td>Draft</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>puddingpiler_match_details_NA1_4246887442.json</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>13868</td>\n",
       "      <td>Draft</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         match_id  kills  deaths  assists  \\\n",
       "0                              .ipynb_checkpoints      5       7       17   \n",
       "1  puddingpiler_match_details_NA1_4245705629.json      5       7       17   \n",
       "2  puddingpiler_match_details_NA1_4245731071.json      5       7       17   \n",
       "3  puddingpiler_match_details_NA1_4246871711.json      5       7       17   \n",
       "4  puddingpiler_match_details_NA1_4246887442.json      5       7       17   \n",
       "\n",
       "   gold_earned match_type    win  \n",
       "0        13868      Draft  False  \n",
       "1        13868      Draft  False  \n",
       "2        13868      Draft  False  \n",
       "3        13868      Draft  False  \n",
       "4        13868      Draft  False  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the win column of booleans with 1 or 0 instead of the True/False values given\n",
    "#1 indacates a win and 0 indicates loss\n",
    "df['win'] = df['win'].apply(lambda x: 1 if x == True else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating Game Modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to separate all the different game modes from each other (ARAM, Ranked, etc.) so that we may do data analysis on data from one game type at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Draft    11\n",
       "Name: match_type, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting a count of what kind and how many matches there are\n",
    "df['match_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 11 0 0\n"
     ]
    }
   ],
   "source": [
    "#Creating dataframes of gametypes\n",
    "#Dropping the match_type columns since it will be redundant with the name of the dataframe\n",
    "\n",
    "ARAM = df[df['match_type'] == 'ARAM'].drop(['match_type'], axis = 1)\n",
    "Draft = df[df['match_type'] == 'Draft'].drop(['match_type'], axis = 1)\n",
    "ARURF = df[df['match_type'] == 'ARURF'].drop(['match_type'], axis = 1)\n",
    "Ranked = df[df['match_type'] == 'Ranked'].drop(['match_type'], axis = 1)\n",
    "\n",
    "\n",
    "#Checking if data separated correctly\n",
    "print (len(ARAM), len(Draft), len(ARURF), len(Ranked))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Out Dataframe of Ranked games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_id</th>\n",
       "      <th>kills</th>\n",
       "      <th>deaths</th>\n",
       "      <th>assists</th>\n",
       "      <th>gold_earned</th>\n",
       "      <th>win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [match_id, kills, deaths, assists, gold_earned, win]\n",
       "Index: []"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ranked.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Wins using a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to answer the questions: \n",
    "- Can we predict if someone will win based on user metrics, such as kills, deaths, etc.?\n",
    "- What independent variables are good predictors for a win or lose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start answering these questions by using a logistic regression model to predict whether or not a player wins or loses where the dependent variables are those given in the dataframe created earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we find that these independent variables are not good predictors, then we can always go back and insert new types of variables from match data into our logistic regression model as an extension of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build any machine learning model, we must divide our avaiable data into three datasets:\n",
    "- training dataset : Used to train the model to predict outcomes\n",
    "- testing dataset : Data used to test the model created through the training dataset. Can tweak until we have reasonable predictions\n",
    "- validation dataset : Final testing dataset for the model. If the model does not predict well enough, then we start over with our model. Either toss the model, or review if you made enough reasonable changes to your model in the first place in the testing phase.\n",
    "\n",
    "We will not be validating this dataset as I believe there is not enough data with 100 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DELETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kills</th>\n",
       "      <th>deaths</th>\n",
       "      <th>assists</th>\n",
       "      <th>gold_earned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [kills, deaths, assists, gold_earned]\n",
       "Index: []"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ARAM[['kills', 'deaths', 'assists', 'gold_earned']]\n",
    "y = ARAM['win']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kills</th>\n",
       "      <th>deaths</th>\n",
       "      <th>assists</th>\n",
       "      <th>gold_earned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [kills, deaths, assists, gold_earned]\n",
       "Index: []"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: win, dtype: int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-c88ade6684c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#We will split our dataframe to train and test our model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2129\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2130\u001b[1;33m     n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size,\n\u001b[0m\u001b[0;32m   2131\u001b[0m                                               default_test_size=0.25)\n\u001b[0;32m   2132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   1808\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1809\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1810\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   1811\u001b[0m             \u001b[1;34m'With n_samples={}, test_size={} and train_size={}, the '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1812\u001b[0m             \u001b[1;34m'resulting train set will be empty. Adjust any of the '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#We will split our dataframe to train and test our model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-88f28d4956a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#We will split our dataframe to train and test our model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2129\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2130\u001b[1;33m     n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size,\n\u001b[0m\u001b[0;32m   2131\u001b[0m                                               default_test_size=0.25)\n\u001b[0;32m   2132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   1808\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1809\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1810\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   1811\u001b[0m             \u001b[1;34m'With n_samples={}, test_size={} and train_size={}, the '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1812\u001b[0m             \u001b[1;34m'resulting train set will be empty. Adjust any of the '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "#X will be the independent variables for our model given by kda and gold earned\n",
    "#y will be the independent variable, which is categorical and indacates a win or loss\n",
    "X = Ranked[['kills', 'deaths', 'assists', 'gold_earned']]\n",
    "y = Ranked['win']\n",
    "\n",
    "#We will split our dataframe to train and test our model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an Instance of the Logistic regression model form scikit-learn\n",
    "LogModel = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kills</th>\n",
       "      <th>deaths</th>\n",
       "      <th>assists</th>\n",
       "      <th>gold_earned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>15482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>15482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   kills  deaths  assists  gold_earned\n",
       "0     11      10       14        15482\n",
       "1     11      10       14        15482"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-c37d953d7aa4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Fitting the training data onto our logistic regression model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mLogModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1370\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1372\u001b[1;33m             raise ValueError(\"This solver needs samples of at least 2 classes\"\n\u001b[0m\u001b[0;32m   1373\u001b[0m                              \u001b[1;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m                              \" class: %r\" % classes_[0])\n",
      "\u001b[1;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1"
     ]
    }
   ],
   "source": [
    "#Fitting the training data onto our logistic regression model\n",
    "LogModel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting from our test data whether we win or lose a game\n",
    "y_predict = LogModel.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We can analyze the model predictions a few different ways. Here we will be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = x = range(len(y_predict)), y = np.abs(y_predict - y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 indicates an incorrect prediction of win or lose, \n",
    "#0 indicates a correct prediction of win or lose\n",
    "sns.scatterplot(x = range(len(y_predict)), y = np.abs(y_predict - y_test))\n",
    "plt.ylim(-.1,1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Confusion matrix giving:\n",
    "#    True Positive    False Positive\n",
    "#    False Negative   True Negative\n",
    "confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
